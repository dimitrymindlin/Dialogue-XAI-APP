xai_explanations:
  - explanation_name: "ModelPredictionConfidence"
    description: "This explanation provides information about the model's confidence in its prediction."
    explanation_steps:
      - step_name: "Concept"
        description: >
          The model prediction confidence explanation shows how certain the model is in its decision, influenced by dataset features. Confidence can be high or low.
      - step_name: "Confidence"
        description: >
          {{model_confidence}}

  - explanation_name: "FeatureImportances"
    description: "Explains how each feature of the current instance influences the prediction, showing positive or negative contributions to understand the importance and influence of features in the model’s decision‐making."
    explanation_steps:
      - step_name: "Concept"
        description: >
          Explain that each feature can push the prediction toward one of the classes in different amounts. Negative values pull toward {{negative_class}} and positive values toward {{possible_classes}}, with larger magnitudes indicating stronger effects. After listing the top drivers, offer to show a plot of these influences.
      - step_name: "FeatureInfluencesPlot"
        description: >
          Reading plots requires some familiarity with data science or statistics. Consider avoiding them for lay users unless specifically requested or clearly appropriate.
          The plot visualizes feature importance and each feature's influence on the model's decision. Blue bars indicate features pushing the prediction toward {{class_0_description}} - the more negative the value, the stronger the influence. Red bars indicate influence toward {{class_1_description}}. Since the plot lacks explanations, be sure to provide context if necessary.
          Use the placeholder ##FeatureInfluencesPlot## to integrate the chart smoothly into the explanation—it will be replaced by the actual plot.
          The base value of this SHAP explanation is {{shap_base_value}}, indicating an initial bias toward the {{shap_initial_bias}} class. This may be important to mention depending on the context.
      - step_name: "FeaturesInFavourOfClass1"
        description: >
          The base value of this SHAP explanation is {{shap_base_value}} in favor of class {{shap_initial_bias}}.
          {{feature_importance.features_in_favour_of_class_1}}
      - step_name: "FeaturesInFavourOfClass0"
        description: >
          The base value of this SHAP explanation is {{shap_base_value}} in favor of class {{shap_initial_bias}}.
          {{feature_importance.features_in_favour_of_class_0}}
      - step_name: "WhyThisFeatureImportant"
        description: >
          Check PossibleClarifications for a reply to this question. We cannot address why a feature is specially important
          beyond explaining how features influence the model's decision and how the importances are learned during training and observing many examples.

  - explanation_name: "Counterfactuals"
    description: "Answers questions about how to change the instance to get a different prediction."
    explanation_steps:
      - step_name: "Concept"
        description: >
          Counterfactual explanations highlight feature changes that alter a model’s prediction. The method prioritizes minimal counterfactuals, modifying only one attribute.
      - step_name: "PossibleChanges"
        description: >
          {{counterfactuals}}

  - explanation_name: "AnchorExplanation"
    description: "Finding the minimal set of features that define or anchor the prediction."
    explanation_steps:
      - step_name: "Concept"
        description: >
          Anchor explanations identify the minimal set of features that anchor the model’s decision. They provide a clear, human-understandable explanation for the model’s prediction that is similar to the most important features.
      - step_name: "Anchor"
        description: >
          Anchor showing which features with their current values need to be present for the model to predict the current model prediction. {{anchor.anchor_text}}

  - explanation_name: "CeterisParibus"
    description: "Understanding the model's local decision-making by observing how the model's prediction changes when only one feature is changed. Similar to TextualPartialDependence, but focused on individual instances. Explain this difference in simple terms to the user."
    explanation_steps:
      - step_name: "Concept"
        description: >
          Ceteris Paribus explanations show how changing one feature affects the model’s prediction while keeping others constant, revealing individual feature impact.
      - step_name: "PossibleClassFlips"
        description: >
          {{ceteris_paribus.possible_class_flips}}
      - step_name: "ImpossibleClassFlips"
        description: >
          {{ceteris_paribus.impossible_class_flips}}

  - explanation_name: "TextualPartialDependence"
    description: "Understanding the global relationship between a feature and the model's prediction. Similar to Ceteris Paribus but focused on the global model behavior. They can be used together to describe geenral trends but also local influences for the current datapoint. Explain this difference in simple terms to the user."
    explanation_steps:
      - step_name: "Concept"
        description: >
          Textual PDP explanations describe how a feature affects the model’s prediction while keeping others constant. They highlight global relationships and how predictions change at specific feature values.
      - step_name: "PDPDescription"
        description: >
          {{pdp.all_pdp_text}}

  - explanation_name: "FeatureStatistics"
    description: "An overview of the statistical distribution of features used in the model, such as minimum and maximum values of features, the mean and occurrences of categories."
    explanation_steps:
      - step_name: "Concept"
        description: >
          A statistical breakdown of features (ranges, means, distributions) helps understand the dataset, revealing imbalances, value ranges, and categorical options.
      - step_name: "Feature Statistics"
        description: >
          {{feature_statistics.feature_statistics}}

  - explanation_name: "PossibleClarifications"
    description: "Here are some clarification with common user questions."
    explanation_steps:
      - step_name: "Concept"
        description: >
          Possible clarifications provide answers to common user questions and help in understanding the explanations better.
      - step_name: "ClarificationsList"
        description: >
          {{static_clarifications.all_clarifications}}

  - explanation_name: "ScaffoldingStrategy"
    description: "A strategy for improving understanding by simplifying, repeating, and eliciting feedback."
    explanation_steps:
      - step_name: "Reformulating"
        description: >
          Reformulating the explanation in a simpler way.

      - step_name: "Repeating"
        description: >
          Repeating the explanation slightly differently to reinforce understanding.

      - step_name: "ElicitingFeedback"
        description: >
          A question prompting the user to summarize or restate key parts of the explanation in their own words. It assesses understanding and identifies areas needing clarification based on conversation history and context.